# llm-finetuning-cookbook

Planned Directory Structure

```
/llm-finetuning-cookbook
|-- Part_0_Introduction_and_Setup
|   |-- 00_Introduction_and_Setup.ipynb
|   |-- 00b_Data_Curation_and_Formatting.ipynb
|-- Part_1_Supervised_Finetuning_SFT
|   |-- 01_SFT_with_TRL_and_LoRA.ipynb
|   |-- 01b_SFT_for_Structured_Output_JSON.ipynb
|   |-- 02_Full_Parameter_SFT_with_TRL.ipynb
|   |-- 03_Faster_SFT_with_Unsloth.ipynb
|-- Part_2_Preference_Alignment
|   |-- 04_Aligning_with_DPO_using_TRL.ipynb
|   |-- 05_Simplified_Alignment_with_ORPO.ipynb
|   |-- 06_Advanced_RL_Alignment_with_GRPO.ipynb
|-- Part_3_Advanced_Finetuning_with_Axolotl
|   |-- 07_Reproducible_Finetuning_with_Axolotl.ipynb
|   |-- 08_Continued_Pretraining_with_Axolotl.ipynb
|-- Part_4_Quantization
|   |-- 09_Post_Training_Quantization_for_GGUF_llama_cpp.ipynb
|   |-- 10_Advanced_PTQ_with_AutoRound.ipynb
|   |-- 11_Quantization_Aware_Training_QAT.ipynb
|-- Part_5_Deployment_with_vLLM
|   |-- 12_Deploying_a_Merged_Model_with_vLLM.ipynb
|   |-- 13_Deploying_with_LoRA_Adapters_vLLM.ipynb
|   |-- 14_Deploying_Quantized_Models_vLLM.ipynb
|-- Part_6_Evaluation
|   |-- 15_Benchmarking_with_LM_Evaluation_Harness.ipynb
|-- Part_7_Advanced_Architectures
|   |-- 16_Finetuning_a_Mixture_of_Experts_MoE.ipynb
|   |-- 17_Finetuning_a_Vision_Language_Model_VLM.ipynb
|   |-- 18_Merging_LoRA_Adapters.ipynb
|-- Appendix
|   |-- Debugging.md
|   |-- Common_Pitfalls.ipynb
|-- /scripts
|-- /data
|-- requirements.txt
|-- README.md
```
